{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f4c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603791e",
   "metadata": {},
   "source": [
    "# 1 \n",
    "My initial webpage is an ESPN article on a hard hit that occurred in the NFL. I request the html, then parse the contents using beautifulsoup. I extract all the URLs and then append them to the urls array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8d6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.espn.com/nfl/story/_/id/34761662/dolphins-teddy-bridgewater-protocol-hard-hit'\n",
    "\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "wikipediaCount = 0\n",
    "urls = []\n",
    "for text in soup.find_all('div'):\n",
    "    for links in text.find_all('a'):\n",
    "        link = links.get('href')\n",
    "        if link is not None and link[:8] == 'https://':\n",
    "            urls.append(link)\n",
    "#urls = urls[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131596d",
   "metadata": {},
   "source": [
    "# 2\n",
    "The following cell loops through all the previously generated URLs and scrapes text off each page. The data is then pickled and dumped into the \"data\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fd9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = {}\n",
    "for url in urls:\n",
    "    data[url] = []\n",
    "    p = requests.get(url)\n",
    "    soup2 = BeautifulSoup(p.content, 'html.parser')\n",
    "    for post in soup2.find_all('p'):\n",
    "        data[url].append(post.get_text())\n",
    "    pickle.dump(data[url], open(f'data/{hash(url)}.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd425256",
   "metadata": {},
   "source": [
    "# 3\n",
    "The following cell cleans up the text for each file and outputs a pickle file in the clean_data directory. In order to remove whitespace from the corpus, I use a regular expression to replace all whitespace with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7f0fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = {}\n",
    "for url, val in data.items():\n",
    "    sentences[url] = []\n",
    "    for text in val:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.lower()\n",
    "        sentences[url].extend(sent_tokenize(text))\n",
    "    pickle.dump(data[url], open(f'clean_data/{hash(url)}.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac013885",
   "metadata": {},
   "source": [
    "# 4\n",
    "The following cell parses the data we've extracted. I first add every sentence in our dataset to the corpus. Then, I convert the corpus to tokens and filter out any tokens less than length 4. I noticed that the terms of service at the bottom of the website heavily skew the tokens towards words in the terms of service, so I added this string to a whitelist filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af87e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('products', 90),\n",
       " ('content', 55),\n",
       " ('united', 54),\n",
       " ('walt', 53),\n",
       " ('including', 45),\n",
       " ('agreement', 41),\n",
       " ('access', 38),\n",
       " ('email', 36),\n",
       " ('child', 36),\n",
       " ('product', 34),\n",
       " ('provide', 33),\n",
       " ('and/or', 32),\n",
       " ('digital', 32),\n",
       " ('company', 31),\n",
       " ('also', 31),\n",
       " ('agree', 31),\n",
       " ('consent', 31),\n",
       " ('services', 29),\n",
       " ('limited', 29),\n",
       " ('applications', 29),\n",
       " ('parent', 29),\n",
       " ('address', 29),\n",
       " ('family', 28),\n",
       " ('certain', 28),\n",
       " ('right', 27),\n",
       " ('third', 27),\n",
       " ('entertainment', 26),\n",
       " ('sites', 26),\n",
       " ('collect', 25),\n",
       " ('fantasy', 22),\n",
       " ('make', 22),\n",
       " ('available', 22),\n",
       " ('user', 22),\n",
       " ('generated', 22),\n",
       " ('notice', 22),\n",
       " ('please', 21),\n",
       " ('dispute', 21),\n",
       " ('media', 21),\n",
       " ('individual', 20),\n",
       " ('third-party', 20)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "corpus = \"\"\n",
    "for url, sents in sentences.items():\n",
    "    for sent in sents:\n",
    "        corpus += sent \n",
    "\n",
    "EULA = \"Terms of Use Privacy Policy Your California Privacy Rights Children Online Privacy Policy Interest Based Ads About Nielsen Measurement Do Not Sell My Personal Information Contact Us Disney Ad Sales Site Work for ESPN Copyright : © ESPN Enterprises Inc. All rights reserved\"\n",
    "EULA = EULA.lower()\n",
    "        \n",
    "word_filter = set(stopwords.words('english') + list(string.punctuation) + EULA.split(' '))\n",
    "tokens = [t for t in word_tokenize(corpus) if len(t) > 3 and t not in word_filter]\n",
    "counts = Counter(tokens)\n",
    "counts.most_common(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c16cb",
   "metadata": {},
   "source": [
    "# 5\n",
    "The most important words are as follows:\n",
    "\n",
    "game, required, digital, dispute, york, south, tagovailoa, concussion, united, subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f8fc4",
   "metadata": {},
   "source": [
    "# 6\n",
    "In the following cell, I create the knowledge base described in the attached knowledge base document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e19669",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['game', 'required', 'digital', 'dispute', 'york', 'south', 'tagovailoa', 'concussion', 'united', 'subject']\n",
    "knowledge_base = {}\n",
    "for word in words:\n",
    "    #Find every occurrence of the word in the corpus\n",
    "    res = [i.start() for i in re.finditer(word, corpus)]\n",
    "    #Add the containing sentence to the knowledge base\n",
    "    # Substring from previous period to the next period\n",
    "    knowledge_base[word] = [corpus[corpus[:p].rfind('.')+1: corpus.find('.', p+1)] \\\n",
    "                              for p in res]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb9d4f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun fact: a comprehensive listing of the elements that have been added to our fantasy game for the upcoming season!\n",
      "\n",
      "Fun fact: paul finebaum and keyshawn johnson break down why the dolphins will move on from tua tagovailoa if a better option at quarterback comes up\n",
      "\n",
      "Fun fact: bridgewater was put into the protocol after the booth atc spotter ruled him a \"no-go\" after he took a hit on the dolphins' opening offensive drive, in compliance with the nfl's amended concussion protocol\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fun fact: {knowledge_base['game'][2]}!\\n\")\n",
    "print(f\"Fun fact: {knowledge_base['tagovailoa'][0]}!\\n\")\n",
    "print(f\"Fun fact: {knowledge_base['concussion'][1]}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb9349ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun fact: a comprehensive listing of the elements that have been added to our fantasy game for the upcoming season!\n",
      "\n",
      "Fun fact: paul finebaum and keyshawn johnson break down why the dolphins will move on from tua tagovailoa if a better option at quarterback comes up!\n",
      "\n",
      "Fun fact: bridgewater was put into the protocol after the booth atc spotter ruled him a \"no-go\" after he took a hit on the dolphins' opening offensive drive, in compliance with the nfl's amended concussion protocol!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "if __name__=='__main__':\n",
    "\t# 1) My initial webpage is an ESPN article on a hard hit that occurred in the NFL. \n",
    "\t#    I request the html, then parse the contents using beautifulsoup. \n",
    "\t#    I extract all the URLs and then append them to the urls array.\n",
    "\tURL = 'https://www.espn.com/nfl/story/_/id/34761662/dolphins-teddy-bridgewater-protocol-hard-hit'\n",
    "\n",
    "\tpage = requests.get(URL)\n",
    "\tsoup = BeautifulSoup(page.content, 'html.parser')\n",
    "\twikipediaCount = 0\n",
    "\turls = []\n",
    "\tfor text in soup.find_all('div'):\n",
    "\t\tfor links in text.find_all('a'):\n",
    "\t\t\tlink = links.get('href')\n",
    "\t\t\tif link is not None and link[:8] == 'https://':\n",
    "\t\t\t\turls.append(link)\n",
    "\n",
    "\n",
    "\t# 2) The following cell loops through all the previously generated URLs and scrapes text off each page. \n",
    "\t#    The data is then pickled and dumped into the \"data\" directory\n",
    "\tdata = {}\n",
    "\tfor url in urls:\n",
    "\t\tdata[url] = []\n",
    "\t\tp = requests.get(url)\n",
    "\t\tsoup2 = BeautifulSoup(p.content, 'html.parser')\n",
    "\t\tfor post in soup2.find_all('p'):\n",
    "\t\t\tdata[url].append(post.get_text())\n",
    "\t\tpickle.dump(data[url], open(f'data/{hash(url)}.p', 'wb'))\n",
    "\n",
    "\n",
    "\t# 3) The following cell cleans up the text for each file and outputs a pickle file in the clean_data directory. \n",
    "\t#    In order to remove whitespace from the corpus, I use a regular expression to replace all whitespace with a single space\n",
    "\tsentences = {}\n",
    "\tfor url, val in data.items():\n",
    "\t\tsentences[url] = []\n",
    "\t\tfor text in val:\n",
    "\t\t\ttext = re.sub(r'\\s+', ' ', text)\n",
    "\t\t\ttext = text.lower()\n",
    "\t\t\tsentences[url].extend(sent_tokenize(text))\n",
    "\t\tpickle.dump(data[url], open(f'clean_data/{hash(url)}.p', 'wb'))\n",
    "\n",
    "\n",
    "\t# 4) The following cell parses the data we've extracted. I first add every sentence in our dataset to the corpus. \n",
    "\t#    Then, I convert the corpus to tokens and filter out any tokens less than length 4. \n",
    "\t#    I noticed that the terms of service at the bottom of the website heavily skew the tokens towards words in the terms of service, so I added this string to a whitelist filter\n",
    "\tcorpus = \"\"\n",
    "\tfor url, sents in sentences.items():\n",
    "\t\tfor sent in sents:\n",
    "\t\t\tcorpus += sent \n",
    "\n",
    "\tEULA = \"Terms of Use Privacy Policy Your California Privacy Rights Children Online Privacy Policy Interest Based Ads About Nielsen Measurement Do Not Sell My Personal Information Contact Us Disney Ad Sales Site Work for ESPN Copyright : © ESPN Enterprises Inc. All rights reserved\"\n",
    "\tEULA = EULA.lower()\n",
    "\t\t\t\n",
    "\tword_filter = set(stopwords.words('english') + list(string.punctuation) + EULA.split(' '))\n",
    "\ttokens = [t for t in word_tokenize(corpus) if len(t) > 3 and t not in word_filter]\n",
    "\tcounts = Counter(tokens)\n",
    "\tcounts.most_common(40)\n",
    "\n",
    "\t# 5) The most important words are as follows:\n",
    "\t# game, required, digital, dispute, york, south, tagovailoa, concussion, united, subject\n",
    "\t\n",
    "\t# 6) I then create the knowledge base.\n",
    "\twords = ['game', 'required', 'digital', 'dispute', 'york', 'south', 'tagovailoa', 'concussion', 'united', 'subject']\n",
    "\tknowledge_base = {}\n",
    "\tfor word in words:\n",
    "\t\t#Find every occurrence of the word in the corpus\n",
    "\t\tres = [i.start() for i in re.finditer(word, corpus)]\n",
    "\t\t#Add the containing sentence to the knowledge base\n",
    "\t\t# Substring from previous period to the next period\n",
    "\t\tknowledge_base[word] = [corpus[corpus[:p].rfind('.')+1: corpus.find('.', p+1)] \\\n",
    "\t\t\t\t\t\t\t\t  for p in res]\n",
    "\n",
    "\t# Test KB\n",
    "\tprint(f\"Fun fact: {knowledge_base['game'][2]}!\\n\")\n",
    "\tprint(f\"Fun fact: {knowledge_base['tagovailoa'][0]}!\\n\")\n",
    "\tprint(f\"Fun fact: {knowledge_base['concussion'][1]}!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
