{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23f4c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603791e",
   "metadata": {},
   "source": [
    "# 1 \n",
    "My initial webpage is an ESPN article on a hard hit that occurred in the NFL. I request the html, then parse the contents using beautifulsoup. I extract all the URLs and then append them to the urls array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab8d6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.espn.com/nfl/story/_/id/34761662/dolphins-teddy-bridgewater-protocol-hard-hit'\n",
    "\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "wikipediaCount = 0\n",
    "urls = []\n",
    "for text in soup.find_all('div'):\n",
    "    for links in text.find_all('a'):\n",
    "        link = links.get('href')\n",
    "        if link is not None and link[:8] == 'https://':\n",
    "            urls.append(link)\n",
    "#urls = urls[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131596d",
   "metadata": {},
   "source": [
    "# 2\n",
    "The following cell loops through all the previously generated URLs and scrapes text off each page. The data is then pickled and dumped into the \"data\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12fd9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = {}\n",
    "for url in urls:\n",
    "    data[url] = []\n",
    "    p = requests.get(url)\n",
    "    soup2 = BeautifulSoup(p.content, 'html.parser')\n",
    "    for post in soup2.find_all('p'):\n",
    "        data[url].append(post.get_text())\n",
    "    pickle.dump(data[url], open(f'data/{hash(url)}.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd425256",
   "metadata": {},
   "source": [
    "# 3\n",
    "The following cell cleans up the text for each file and outputs a pickle file in the clean_data directory. In order to remove whitespace from the corpus, I use a regular expression to replace all whitespace with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e7f0fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = {}\n",
    "for url, val in data.items():\n",
    "    sentences[url] = []\n",
    "    for text in val:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.lower()\n",
    "        sentences[url].extend(sent_tokenize(text))\n",
    "    pickle.dump(data[url], open(f'clean_data/{hash(url)}.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac013885",
   "metadata": {},
   "source": [
    "# 4\n",
    "The following cell parses the data we've extracted. I first add every sentence in our dataset to the corpus. Then, I convert the corpus to tokens and filter out any tokens less than length 4. I noticed that the terms of service at the bottom of the website heavily skew the tokens towards words in the terms of service, so I added this string to a whitelist filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9af87e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('products', 90),\n",
       " ('content', 55),\n",
       " ('united', 54),\n",
       " ('walt', 53),\n",
       " ('including', 45),\n",
       " ('agreement', 41),\n",
       " ('access', 38),\n",
       " ('email', 36),\n",
       " ('child', 36),\n",
       " ('product', 34),\n",
       " ('provide', 33),\n",
       " ('and/or', 32),\n",
       " ('digital', 32),\n",
       " ('company', 31),\n",
       " ('also', 31),\n",
       " ('agree', 31),\n",
       " ('consent', 31),\n",
       " ('services', 29),\n",
       " ('limited', 29),\n",
       " ('applications', 29),\n",
       " ('parent', 29),\n",
       " ('address', 29),\n",
       " ('family', 28),\n",
       " ('certain', 28),\n",
       " ('right', 27),\n",
       " ('third', 27),\n",
       " ('entertainment', 26),\n",
       " ('sites', 26),\n",
       " ('collect', 25),\n",
       " ('fantasy', 22),\n",
       " ('make', 22),\n",
       " ('available', 22),\n",
       " ('user', 22),\n",
       " ('generated', 22),\n",
       " ('notice', 22),\n",
       " ('please', 21),\n",
       " ('dispute', 21),\n",
       " ('media', 21),\n",
       " ('individual', 20),\n",
       " ('third-party', 20)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "corpus = \"\"\n",
    "for url, sents in sentences.items():\n",
    "    for sent in sents:\n",
    "        corpus += sent \n",
    "\n",
    "EULA = \"Terms of Use Privacy Policy Your California Privacy Rights Children Online Privacy Policy Interest Based Ads About Nielsen Measurement Do Not Sell My Personal Information Contact Us Disney Ad Sales Site Work for ESPN Copyright : Â© ESPN Enterprises Inc. All rights reserved\"\n",
    "EULA = EULA.lower()\n",
    "        \n",
    "word_filter = set(stopwords.words('english') + list(string.punctuation) + EULA.split(' '))\n",
    "tokens = [t for t in word_tokenize(corpus) if len(t) > 3 and t not in word_filter]\n",
    "counts = Counter(tokens)\n",
    "counts.most_common(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c16cb",
   "metadata": {},
   "source": [
    "# 5\n",
    "The most important words are as follows:\n",
    "\n",
    "game, required, digital, dispute, york, south, tagovailoa, concussion, united, subject"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
