{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20bc363",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "This section imports the toolkit and initializes the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682800bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yucjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yucjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yucjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\yucjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6e8e8",
   "metadata": {},
   "source": [
    "# Question 3\n",
    " - I learned that the API is outdated, as text1.tokens is a list, not a method.\n",
    " - I learned that the tokens list is generated when the text object is created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17d6b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.',\n",
       " '(',\n",
       " 'Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import text1\n",
    "text1.tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556c09a",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "This section finds five instances of the word 'sea' and prints it in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0c6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 455 matches:\n",
      " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
      " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
      "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
      "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
      " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
     ]
    }
   ],
   "source": [
    "text1.concordance('sea', lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed40db",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "- This function is simply an abstraction on top of Python’s existing count method. \n",
    "- It uses the built in count method on object’s “tokens” list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f705537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(text1.count('sea'))\n",
    "print('sea sea sea sea lol sea'.count('sea'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ace72",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "This section prints the first 10 elements of a list of tokens of an excerpt of the wikipedia page on NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556edc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Since',\n",
       " 'the',\n",
       " 'so-called',\n",
       " '``',\n",
       " 'statistical',\n",
       " 'revolution',\n",
       " \"''\",\n",
       " '[',\n",
       " '15',\n",
       " ']']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "raw_text = '''Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\n",
    "Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature (complex-valued embeddings,[17] and neural networks in general have also been proposed, for e.g. speech[18]). '''\n",
    "\n",
    "tokens = word_tokenize(raw_text)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778f0bd",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "This section prints the sentence tokenization of the raw_text variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9fce24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning.',\n",
       " 'The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.',\n",
       " 'Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.',\n",
       " 'These algorithms take as input a large set of \"features\" that are generated from the input data.',\n",
       " 'Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature (complex-valued embeddings,[17] and neural networks in general have also been proposed, for e.g.',\n",
       " 'speech[18]).']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sent_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd9211",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "This section stems all the words in the tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77bf51ca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sinc',\n",
       " 'the',\n",
       " 'so-cal',\n",
       " '``',\n",
       " 'statist',\n",
       " 'revolut',\n",
       " \"''\",\n",
       " '[',\n",
       " '15',\n",
       " ']',\n",
       " '[',\n",
       " '16',\n",
       " ']',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980',\n",
       " 'and',\n",
       " 'mid-1990',\n",
       " ',',\n",
       " 'much',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'research',\n",
       " 'ha',\n",
       " 'reli',\n",
       " 'heavili',\n",
       " 'on',\n",
       " 'machin',\n",
       " 'learn',\n",
       " '.',\n",
       " 'the',\n",
       " 'machine-learn',\n",
       " 'paradigm',\n",
       " 'call',\n",
       " 'instead',\n",
       " 'for',\n",
       " 'use',\n",
       " 'statist',\n",
       " 'infer',\n",
       " 'to',\n",
       " 'automat',\n",
       " 'learn',\n",
       " 'such',\n",
       " 'rule',\n",
       " 'through',\n",
       " 'the',\n",
       " 'analysi',\n",
       " 'of',\n",
       " 'larg',\n",
       " 'corpora',\n",
       " '(',\n",
       " 'the',\n",
       " 'plural',\n",
       " 'form',\n",
       " 'of',\n",
       " 'corpu',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'document',\n",
       " ',',\n",
       " 'possibl',\n",
       " 'with',\n",
       " 'human',\n",
       " 'or',\n",
       " 'comput',\n",
       " 'annot',\n",
       " ')',\n",
       " 'of',\n",
       " 'typic',\n",
       " 'real-world',\n",
       " 'exampl',\n",
       " '.',\n",
       " 'mani',\n",
       " 'differ',\n",
       " 'class',\n",
       " 'of',\n",
       " 'machine-learn',\n",
       " 'algorithm',\n",
       " 'have',\n",
       " 'been',\n",
       " 'appli',\n",
       " 'to',\n",
       " 'natural-language-process',\n",
       " 'task',\n",
       " '.',\n",
       " 'these',\n",
       " 'algorithm',\n",
       " 'take',\n",
       " 'as',\n",
       " 'input',\n",
       " 'a',\n",
       " 'larg',\n",
       " 'set',\n",
       " 'of',\n",
       " '``',\n",
       " 'featur',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'are',\n",
       " 'gener',\n",
       " 'from',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data',\n",
       " '.',\n",
       " 'increasingli',\n",
       " ',',\n",
       " 'howev',\n",
       " ',',\n",
       " 'research',\n",
       " 'ha',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'statist',\n",
       " 'model',\n",
       " ',',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft',\n",
       " ',',\n",
       " 'probabilist',\n",
       " 'decis',\n",
       " 'base',\n",
       " 'on',\n",
       " 'attach',\n",
       " 'real-valu',\n",
       " 'weight',\n",
       " 'to',\n",
       " 'each',\n",
       " 'input',\n",
       " 'featur',\n",
       " '(',\n",
       " 'complex-valu',\n",
       " 'embed',\n",
       " ',',\n",
       " '[',\n",
       " '17',\n",
       " ']',\n",
       " 'and',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'in',\n",
       " 'gener',\n",
       " 'have',\n",
       " 'also',\n",
       " 'been',\n",
       " 'propos',\n",
       " ',',\n",
       " 'for',\n",
       " 'e.g',\n",
       " '.',\n",
       " 'speech',\n",
       " '[',\n",
       " '18',\n",
       " ']',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "[stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38291db",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "- create-created\n",
    "- enemi-enemy\n",
    "- ani-any\n",
    "- mani-many\n",
    "- realiz-realize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2977b526",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Since',\n",
       " 'the',\n",
       " 'so-called',\n",
       " '``',\n",
       " 'statistical',\n",
       " 'revolution',\n",
       " \"''\",\n",
       " '[',\n",
       " '15',\n",
       " ']',\n",
       " '[',\n",
       " '16',\n",
       " ']',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'and',\n",
       " 'mid-1990s',\n",
       " ',',\n",
       " 'much',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'research',\n",
       " 'ha',\n",
       " 'relied',\n",
       " 'heavily',\n",
       " 'on',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'The',\n",
       " 'machine-learning',\n",
       " 'paradigm',\n",
       " 'call',\n",
       " 'instead',\n",
       " 'for',\n",
       " 'using',\n",
       " 'statistical',\n",
       " 'inference',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'such',\n",
       " 'rule',\n",
       " 'through',\n",
       " 'the',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'large',\n",
       " 'corpus',\n",
       " '(',\n",
       " 'the',\n",
       " 'plural',\n",
       " 'form',\n",
       " 'of',\n",
       " 'corpus',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'document',\n",
       " ',',\n",
       " 'possibly',\n",
       " 'with',\n",
       " 'human',\n",
       " 'or',\n",
       " 'computer',\n",
       " 'annotation',\n",
       " ')',\n",
       " 'of',\n",
       " 'typical',\n",
       " 'real-world',\n",
       " 'example',\n",
       " '.',\n",
       " 'Many',\n",
       " 'different',\n",
       " 'class',\n",
       " 'of',\n",
       " 'machine-learning',\n",
       " 'algorithm',\n",
       " 'have',\n",
       " 'been',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'natural-language-processing',\n",
       " 'task',\n",
       " '.',\n",
       " 'These',\n",
       " 'algorithm',\n",
       " 'take',\n",
       " 'a',\n",
       " 'input',\n",
       " 'a',\n",
       " 'large',\n",
       " 'set',\n",
       " 'of',\n",
       " '``',\n",
       " 'feature',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'are',\n",
       " 'generated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data',\n",
       " '.',\n",
       " 'Increasingly',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'research',\n",
       " 'ha',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'model',\n",
       " ',',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft',\n",
       " ',',\n",
       " 'probabilistic',\n",
       " 'decision',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real-valued',\n",
       " 'weight',\n",
       " 'to',\n",
       " 'each',\n",
       " 'input',\n",
       " 'feature',\n",
       " '(',\n",
       " 'complex-valued',\n",
       " 'embeddings',\n",
       " ',',\n",
       " '[',\n",
       " '17',\n",
       " ']',\n",
       " 'and',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'in',\n",
       " 'general',\n",
       " 'have',\n",
       " 'also',\n",
       " 'been',\n",
       " 'proposed',\n",
       " ',',\n",
       " 'for',\n",
       " 'e.g',\n",
       " '.',\n",
       " 'speech',\n",
       " '[',\n",
       " '18',\n",
       " ']',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7c4bd",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "    The NLTK library has an incredibly robust array of functions for operating on and parsing language. I think that it's going to be incredibly useful for everything that we do in the coming semester. I think that the code is of incredibly high quality. The NLTK library is an open-source project, so the code is held to very high standards. I can use NLTK to perform sentiment analysis, and I could also use it to train a simple chat bot. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
