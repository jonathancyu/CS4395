{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffb7b7b",
   "metadata": {},
   "source": [
    "# 1\n",
    "Wordnet is a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1178e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bcfc41",
   "metadata": {},
   "source": [
    "# 2\n",
    "The following cell gets the synsets of the word \"dog\", selects the synset \"cad.n.01\", then prints its definition, examples, and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efc5bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: Synset('good.n.02')\n"
     ]
    }
   ],
   "source": [
    "synsets = wn.synsets(\"good\")\n",
    "synset = synsets[1]\n",
    "\n",
    "print(f\"Synset: {synset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248b8f7",
   "metadata": {},
   "source": [
    "# 3\n",
    "It seems like the wordnet hierarchy is ordered from top to bottom. At the bottom, there are specific words. As you go further up, the hypernyms encapsulate everything until you reach the highest level, entity. There are many more nouns at the bottom of the hierarchy than the top. At the highest level of all nouns is the \"entity\" synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce1bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: moral excellence or admirableness\n",
      "Examples: ['there is much good to be found in people']\n",
      "Lemmas: [Lemma('good.n.02.good'), Lemma('good.n.02.goodness')]\n",
      "[Synset('good.n.02')]\n",
      "[Synset('morality.n.01')]\n",
      "[Synset('quality.n.01')]\n",
      "[Synset('attribute.n.02')]\n",
      "[Synset('abstraction.n.06')]\n",
      "[Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Definition: {synset.definition()}\")\n",
    "print(f\"Examples: {synset.examples()}\")\n",
    "print(f\"Lemmas: {synset.lemmas()}\")\n",
    "\n",
    "current = [synset]\n",
    "while len(current) > 0:\n",
    "        print(current)\n",
    "        current = current[0].hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e588178",
   "metadata": {},
   "source": [
    "# 4 \n",
    "The following cell prints the synset's hypernyms, hyponyms, meronyms, holonyms, and antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94240bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms: [Synset('morality.n.01')]\n",
      "Hyponyms: [Synset('beneficence.n.02'), Synset('benignity.n.01'), Synset('kindness.n.01'), Synset('saintliness.n.01'), Synset('summum_bonum.n.01'), Synset('virtue.n.01'), Synset('virtue.n.04')]\n",
      "Meronyms: []\n",
      "Holonyms: []\n",
      "Antonyms: [Lemma('evil.n.03.evil'), Lemma('evil.n.03.evilness')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hypernyms: {synset.hypernyms()}\")\n",
    "print(f\"Hyponyms: {synset.hyponyms()}\")\n",
    "print(f\"Meronyms: {synset.part_meronyms()}\")\n",
    "print(f\"Holonyms: {synset.part_holonyms()}\")\n",
    "antonyms = []\n",
    "for l in synset.lemmas():\n",
    "    for a in l.antonyms():\n",
    "        antonyms.append(a)\n",
    "print(f\"Antonyms: {antonyms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a87f0b",
   "metadata": {},
   "source": [
    "# 5\n",
    "The following cell outputs all synsets of the given verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d56dd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = wn.synsets(\"jog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549245cd",
   "metadata": {},
   "source": [
    "# 6\n",
    "The following cell will find the hierarchy of the given verb.\n",
    "The hierarchy of verbs in wordnet is similar to the hierarchy of nouns. As you go up the hierarchy from the bottom, the verbs get more general. Unlike the noun hierarchy, the verb hierarchy doesn't originate from the same synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020a66c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('jog.v.03')]\n",
      "[Synset('run.v.01')]\n",
      "[Synset('travel_rapidly.v.01')]\n",
      "[Synset('travel.v.01')]\n"
     ]
    }
   ],
   "source": [
    "current = [verb[5]]\n",
    "while len(current) > 0:\n",
    "        print(current)\n",
    "        current = current[0].hypernyms()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db6d51",
   "metadata": {},
   "source": [
    "# 7\n",
    "The following cell uses morphy to find different forms of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35325c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jog\n",
      "jog\n",
      "jog\n"
     ]
    }
   ],
   "source": [
    "print(wn.morphy('jog', wn.NOUN))\n",
    "print(wn.morphy('jogs', wn.NOUN))\n",
    "print(wn.morphy('jogged', wn.VERB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418261c",
   "metadata": {},
   "source": [
    "# 8\n",
    "The following cell computes the similarity between two words. The Wu-Palmer algorithm shows that lion and tiger are somewhat similar, however they aren't the exact same word. This makes sense, given they are in the same family of words (felines), however they refer to two distinct entities. \n",
    "For the Lesk algorithm, it seems longer sentences result in more context which in turn leads to better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9bc938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5217391304347826\n",
      "Synset('jog.v.06')\n",
      "Synset('square_up.v.02')\n",
      "Synset('trot.v.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "lion = wn.synsets(\"lion\")[0]\n",
    "tiger = wn.synsets(\"tiger\")[0]\n",
    "print(wn.wup_similarity(lion, tiger))\n",
    "\n",
    "print(lesk(\"He jogged to work\".split(), \"jog\"))\n",
    "print(lesk(\"The experience jogged his memory\".split(), \"jog\"))\n",
    "print(lesk(\"He forgot about this, but the experience jogged his memory\".split(), \"jog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4690e7",
   "metadata": {},
   "source": [
    "# 9\n",
    "SentiWordNet is a sentiment analysis dataset for identifying intention and opinion in natural language. It uses three different sentiment scores in order to try to quantify sentiment. SentiWordNet can be used for identifying bias in news articles, or for monitoring polarization on social media.\n",
    "\n",
    "The biggest thing I observed is that each score is completely independent of the context around it. Having these scores can be incredibly useful for NLP applications, however they can be misleading in edge cases such as double negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "133f5abd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive score =  0.125\n",
      "Negative score =  0.0\n",
      "Objective score =  0.875\n",
      "Pos: 0.0, Neg: 0.0\n",
      "Pos: 0.0, Neg: 0.0\n",
      "Pos: 0.0, Neg: 0.0\n",
      "Pos: 0.75, Neg: 0.0\n",
      "Pos: 0.0, Neg: 0.0\n",
      "Pos: 0.0, Neg: 0.0\n",
      "Pos: 0.0, Neg: 0.375\n",
      "Pos: 0.0, Neg: 0.0\n",
      "Pos: 0.125, Neg: 0.0\n",
      "Pos: 0.0, Neg: 0.0\n",
      "Total Pos: 0.375, Total Neg: 0.875\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "word = \"outrage.n.02\"\n",
    "breakdown = swn.senti_synset(word)\n",
    "print(\"Positive score = \", breakdown.pos_score())\n",
    "print(\"Negative score = \", breakdown.neg_score())\n",
    "print(\"Objective score = \", breakdown.obj_score())\n",
    "\n",
    "sentence = \"I was walking my beautiful dog in the ugly park down the street\"\n",
    "neg = 0\n",
    "pos = 0\n",
    "for word in sentence.split():\n",
    "    syn_list = list(swn.senti_synsets(word))\n",
    "    if syn_list:\n",
    "        syn = syn_list[0]\n",
    "        neg += syn.pos_score()\n",
    "        pos += syn.neg_score()\n",
    "        print(f\"Pos: {syn.pos_score()}, Neg: {syn.neg_score()}\")\n",
    "print(f\"Total Pos: {pos}, Total Neg: {neg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbba58",
   "metadata": {},
   "source": [
    "# 10\n",
    "Collocation is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a25110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "0.0 9.975062344139652e-05 9.975062344139652e-05\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-af9702f6b35b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mpmi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp01\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import math\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.book import text4\n",
    "\n",
    "def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return [ngram for ngram in itertools.chain(words, bigrams)\n",
    "                if type(ngram) == tuple]\n",
    "bigram = bigram_word_feats(text4)[3]\n",
    "\n",
    "vocab = len(set(text4))\n",
    "p01 = text4.count(bigram[0] + \" \" + bigram[1])/vocab\n",
    "p0 = text4.count(bigram[0])/vocab\n",
    "p1 = text4.count(bigram[1])/vocab\n",
    "print(p01, p0, p1)\n",
    "pmi = math.log2(p01/(p0*p1))\n",
    "print(pmi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
